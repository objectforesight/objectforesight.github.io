<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ObjectForesight: Predicting 3D Object Trajectories from Human Videos</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        :root {
            --primary-color: #222;
            --secondary-color: #444;
            --accent-color: #4a90d9;
            --venue-color: #e53935;
            --caption-bg: #e8d4f8;
            --light-bg: #f7f8f9;
            --border-radius: 6px;
            --max-width: 950px;
            --content-width: 880px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            font-size: 15px;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        body {
            font-family: 'Noto Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.65;
            color: var(--primary-color);
            background-color: #fff;
            font-weight: 400;
        }

        .container {
            max-width: var(--max-width);
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header Section */
        .header {
            text-align: center;
            padding: 60px 20px 40px;
        }

        .title {
            font-size: 3.2rem;
            font-weight: 600;
            color: var(--primary-color);
            margin-bottom: 12px;
            letter-spacing: -0.5px;
        }

        .subtitle {
            font-size: 1.7rem;
            font-weight: 400;
            color: var(--primary-color);
            margin-bottom: 20px;
            line-height: 1.4;
        }

        .venue {
            font-size: 1.15rem;
            color: var(--venue-color);
            font-weight: 600;
            margin-bottom: 20px;
        }

        .authors {
            margin-bottom: 8px;
            line-height: 1.8;
        }

        .authors a {
            color: var(--accent-color);
            text-decoration: none;
            font-size: 1.2rem;
            font-weight: 400;
        }

        .authors a:hover {
            text-decoration: underline;
        }

        .authors sup {
            font-size: 0.65rem;
            margin-left: 1px;
            color: var(--accent-color);
        }

        .affiliations {
            font-size: 1.1rem;
            color: var(--secondary-color);
            margin-bottom: 5px;
        }

        .affiliations sup {
            font-size: 0.65rem;
        }

        .equal-advising {
            font-size: 0.9rem;
            color: #666;
            margin-bottom: 24px;
        }

        /* Buttons */
        .buttons {
            display: flex;
            justify-content: center;
            gap: 10px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 18px;
            background-color: #363636;
            color: #fff;
            text-decoration: none;
            border-radius: 18px;
            font-size: 0.9rem;
            font-weight: 500;
            transition: all 0.2s ease;
        }

        .btn:hover {
            background-color: #505050;
        }

        .btn i {
            font-size: 0.95rem;
        }

        /* Teaser Section */
        .teaser {
            text-align: center;
            padding: 30px 0 40px;
        }

        .teaser-text {
            font-size: 1rem;
            max-width: var(--content-width);
            margin: 0 auto 28px;
            text-align: center;
            line-height: 1.75;
            color: var(--secondary-color);
        }

        .teaser-text strong {
            font-weight: 600;
            color: var(--primary-color);
        }

        .teaser-video {
            max-width: 850px;
            margin: 0 auto;
        }

        .teaser-video video,
        .teaser-video img {
            width: 100%;
            border-radius: var(--border-radius);
            box-shadow: 0 2px 12px rgba(0,0,0,0.1);
        }

        /* Abstract Section */
        .abstract {
            padding: 25px 0 35px;
        }

        .abstract p {
            max-width: var(--content-width);
            margin: 0 auto;
            font-size: 0.95rem;
            line-height: 1.8;
            text-align: justify;
            color: var(--secondary-color);
        }

        /* Section Headers */
        .section-header {
            text-align: center;
            font-size: 1.6rem;
            font-weight: 500;
            margin: 45px 0 22px;
            color: var(--primary-color);
        }

        /* Architecture Section */
        .architecture {
            padding: 0 0 35px;
        }

        .architecture-description {
            max-width: var(--content-width);
            margin: 0 auto 20px;
            text-align: justify;
            font-size: 0.95rem;
            line-height: 1.75;
            color: var(--secondary-color);
        }

        .architecture-points {
            max-width: var(--content-width);
            margin: 0 auto 30px;
            padding-left: 20px;
            list-style-type: disc;
        }

        .architecture-points li {
            margin-bottom: 10px;
            line-height: 1.7;
            color: var(--secondary-color);
            font-size: 0.95rem;
        }

        .architecture-points li strong {
            color: var(--primary-color);
            font-weight: 600;
        }

        .architecture-figure {
            max-width: 780px;
            margin: 0 auto;
            text-align: center;
        }

        .architecture-figure img,
        .architecture-figure video {
            max-width: 100%;
            border-radius: var(--border-radius);
            box-shadow: 0 2px 12px rgba(0,0,0,0.1);
        }

        .figure-caption {
            font-size: 0.82rem;
            color: #666;
            margin-top: 10px;
            font-style: italic;
        }

        /* Data Curation Section */
        .data-curation {
            padding: 0 0 35px;
        }

        .data-curation-description {
            max-width: var(--content-width);
            margin: 0 auto 25px;
            text-align: justify;
            font-size: 0.95rem;
            line-height: 1.75;
            color: var(--secondary-color);
        }

        .pipeline-figure {
            max-width: 850px;
            margin: 0 auto;
            text-align: center;
        }

        .pipeline-figure img,
        .pipeline-figure video {
            max-width: 100%;
            border-radius: var(--border-radius);
            box-shadow: 0 2px 12px rgba(0,0,0,0.1);
        }

        /* Demo Videos Section */
        .demos {
            padding: 15px 0 40px;
            background-color: var(--light-bg);
        }

        .demos .container {
            max-width: 1050px;
        }

        .demo-category {
            margin-bottom: 30px;
        }

        .demo-category:last-child {
            margin-bottom: 0;
        }

        .demo-category-title {
            font-size: 1.05rem;
            font-weight: 500;
            margin-bottom: 12px;
            color: var(--primary-color);
        }

        .demo-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 18px;
        }

        @media (max-width: 600px) {
            .demo-grid {
                grid-template-columns: 1fr;
            }

            .title {
                font-size: 2rem;
            }

            .subtitle {
                font-size: 1.1rem;
            }
        }

        .demo-item {
            background: #fff;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.07);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .demo-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 16px rgba(0,0,0,0.1);
        }

        .demo-item video {
            width: 100%;
            display: block;
        }

        .demo-caption {
            background: var(--caption-bg);
            padding: 12px 12px;
            text-align: center;
            font-size: 0.85rem;
            color: var(--secondary-color);
            min-height: 60px;
            display: flex;
            align-items: center;
            justify-content: center;
            line-height: 1.45;
        }

        /* Acknowledgements */
        .acknowledgements {
            padding: 35px 0;
        }

        .acknowledgements p {
            max-width: var(--content-width);
            margin: 0 auto;
            text-align: justify;
            line-height: 1.75;
            font-size: 0.95rem;
            color: var(--secondary-color);
        }

        /* BibTeX */
        .bibtex {
            padding: 20px 0 50px;
        }

        .bibtex-block {
            max-width: var(--content-width);
            margin: 0 auto;
            background: #f5f5f5;
            border-radius: var(--border-radius);
            padding: 18px 22px;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            color: var(--secondary-color);
            border: 1px solid #e8e8e8;
        }

        /* Footer */
        .footer {
            text-align: center;
            padding: 25px 20px;
            background: #f0f0f0;
            font-size: 0.85rem;
            color: #666;
            border-top: 1px solid #e0e0e0;
        }

        .footer a {
            color: var(--accent-color);
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        /* Video placeholder styling */
        .video-placeholder {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            width: 100%;
            aspect-ratio: 16/9;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 1rem;
            border-radius: var(--border-radius);
            box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        }

        .video-placeholder i {
            font-size: 2rem;
            margin-bottom: 10px;
            opacity: 0.9;
        }

        .video-placeholder-content {
            text-align: center;
        }

        .video-placeholder small {
            opacity: 0.8;
            font-size: 0.8rem;
        }

        /* Smooth scrolling */
        html {
            scroll-behavior: smooth;
        }

        /* Selection color */
        ::selection {
            background: rgba(93, 173, 226, 0.3);
        }

        /* Pagination styles */
        .demo-tabs {
            display: flex;
            justify-content: center;
            gap: 8px;
            margin-bottom: 25px;
        }

        .demo-tab {
            padding: 8px 20px;
            background: #fff;
            border: 2px solid #ddd;
            border-radius: 20px;
            cursor: pointer;
            font-size: 0.9rem;
            font-weight: 500;
            color: var(--secondary-color);
            transition: all 0.2s ease;
        }

        .demo-tab:hover {
            border-color: var(--accent-color);
            color: var(--accent-color);
        }

        .demo-tab.active {
            background: var(--accent-color);
            border-color: var(--accent-color);
            color: #fff;
        }

        .demo-page {
            display: none;
        }

        .demo-page.active {
            display: block;
        }

        .pagination {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 8px;
            margin-top: 20px;
        }

        .pagination-btn {
            width: 32px;
            height: 32px;
            border-radius: 50%;
            border: 2px solid #ddd;
            background: #fff;
            cursor: pointer;
            font-size: 0.85rem;
            font-weight: 500;
            color: var(--secondary-color);
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .pagination-btn:hover {
            border-color: var(--accent-color);
            color: var(--accent-color);
        }

        .pagination-btn.active {
            background: var(--accent-color);
            border-color: var(--accent-color);
            color: #fff;
        }

        .pagination-btn:disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }
    </style>
</head>
<body>

    <!-- Header -->
    <header class="header">
        <h1 class="title">ObjectForesight</h1>
        <h2 class="subtitle">Predicting 3D Object Trajectories from Human Videos</h2>

        <div class="authors">
            <a href="mailto:rustin@cs.washington.edu">Rustin Soraki</a><sup>1</sup>,
            <a href="#">Homanga Bharadhwaj</a><sup>2*</sup>,
            <a href="#">Ali Farhadi</a><sup>1*</sup>,
            <a href="#">Roozbeh Mottaghi</a><sup>1*</sup>
        </div>
        <p class="affiliations"><sup>1</sup>University of Washington, <sup>2</sup>Carnegie Mellon University</p>
        <p class="equal-advising"><sup>*</sup>Equal Supervision</p>

        <div class="buttons">
            <a href="#" class="btn"><i class="fas fa-file-pdf"></i> Paper</a>
            <a href="#" class="btn"><i class="fas fa-archive"></i> arXiv</a>
            <a href="#" class="btn"><i class="fab fa-github"></i> Code</a>
        </div>
    </header>

    <!-- Teaser -->
    <section class="teaser">
        <div class="container">
            <p class="teaser-text">
                We introduce <strong>ObjectForesight</strong>, a framework for predicting future 3D object trajectories from a video context of past motion.
                We first estimate the object's 3D shape and initial pose, and then predict its future <strong>6D poses</strong> over time.
                There are three key contributions: (1) introducing and formalizing the task of 3D object dynamics prediction from human videos,
                (2) a 3D object-centric dynamics model for future prediction of 6-DoF trajectories,
                (3) a large-scale dataset of <strong>2 million+</strong> object-centric 3D trajectories with pseudo-groundtruth.
            </p>

            <div class="teaser-video">
                <video autoplay muted loop playsinline>
                    <source src="videos/teaser_web.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <!-- Abstract -->
    <section class="abstract">
        <div class="container">
            <p>
                Humans can effortlessly anticipate how objects might move or change through interaction—imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world/dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million+ short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes—establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation.
            </p>

        </div>
    </section>

    <!-- Architecture Section -->
    <section class="architecture">
        <div class="container">
            <h2 class="section-header">Model Architecture</h2>

            <p class="architecture-description">
                ObjectForesight integrates a Diffusion Transformer (DiT) with a geometry-aware 3D point encoder, PointTransformerV3, to jointly reason about object motion and surrounding scene context. Given a short history of RGB frames with corresponding monocular depth maps and a mask of the object in the anchor frame, the model encodes the local 3D geometry of the scene and the object's recent motion into a unified representation. Conditioned on this visual and spatial context, ObjectForesight predicts a distribution over future 6-DoF object poses through a denoising diffusion process.
            </p>

            <ul class="architecture-points">
                <li><strong>Scene and Context Encoding:</strong> A FiLM-conditioned PointTransformerV3 encoder converts the anchor-frame point cloud and motion context into a unified geometric embedding that summarizes local geometry and nearby scene structure.</li>
                <li><strong>Depth-Normalized Pose Tokens:</strong> Each pose is reparameterized into a depth-normalized token representation, which reduces the dynamic range of translation and improves numerical stability in egocentric perspectives.</li>
                <li><strong>Diffusion Transformer:</strong> The model operates on object-centric, depth-normalized 9D pose tokens using v-parameterization with p2 weighting. At inference, DDIM sampling produces smooth, diverse, and physically coherent 3D trajectories.</li>
            </ul>

            <div class="architecture-figure">
                <video autoplay muted loop playsinline>
                    <source src="videos/architecture_web.mp4" type="video/mp4">
                </video>
                <p class="figure-caption">Model architecture: Given past pose tokens, normalized bounding boxes, and the anchor-frame point cloud, ObjectForesight predicts future 6-DoF object trajectories.</p>
            </div>
        </div>
    </section>

    <!-- Data Curation Section -->
    <section class="data-curation">
        <div class="container">
            <h2 class="section-header">Data Curation</h2>

            <p class="data-curation-description">
                Our curation pipeline converts in-the-wild egocentric videos into clean, metrically grounded trajectories of hand-manipulated objects. Starting from EPIC-Kitchens action segments, we apply a sequence of automatic extraction and quality gates to recover temporally coherent 6-DoF poses. We extract 2 million short clips (2–3 seconds each), automatically detecting hands with EgoHOS and identifying objects in contact using SAM2. We then recover 3D object meshes and poses with TRELLIS, and estimate camera motion and monocular depth using SpaTrackerV2. Pose initialization and tracking use FoundationPose with bidirectional tracking and re-registration. This process converts ordinary egocentric videos into the first large-scale dataset of 3D object trajectories at this level of scale, fidelity, and semantic diversity.
            </p>

            <div class="pipeline-figure">
                <video autoplay muted loop playsinline>
                    <source src="videos/datacuration_web.mp4" type="video/mp4">
                </video>
                <p class="figure-caption">Data curation pipeline: From egocentric video to 3D object trajectories using EgoHOS, SAM2, TRELLIS, SpaTrackerV2, and FoundationPose.</p>
            </div>
        </div>
    </section>

    <!-- Demo Videos Section -->
    <section class="demos">
        <div class="container">
            <h2 class="section-header">Qualitative Results</h2>

            <!-- Dataset Tabs -->
            <div class="demo-tabs">
                <button class="demo-tab active" data-dataset="epic">EpicKitchen</button>
                <button class="demo-tab" data-dataset="hot3d">HOT3D</button>
            </div>

            <!-- EpicKitchen Page (1 page, 6 videos) -->
            <div class="demo-page active" data-dataset="epic" data-page="1">
                <div class="demo-grid">
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/epickitchen/epic1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/epickitchen/epic2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/epickitchen/epic3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/epickitchen/epic4.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/epickitchen/epic5.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/epickitchen/epic6.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>

            <!-- HOT3D Page 1 (6 videos) -->
            <div class="demo-page" data-dataset="hot3d" data-page="1">
                <div class="demo-grid">
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d4.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d5.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d6.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="pagination" data-dataset="hot3d">
                    <button class="pagination-btn active" data-page="1">1</button>
                    <button class="pagination-btn" data-page="2">2</button>
                </div>
            </div>

            <!-- HOT3D Page 2 (6 videos) -->
            <div class="demo-page" data-dataset="hot3d" data-page="2">
                <div class="demo-grid">
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d7.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d8.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d9.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d10.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d11.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="demo-item">
                        <video controls muted loop>
                            <source src="videos/hot3d/hot3d12.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="pagination" data-dataset="hot3d">
                    <button class="pagination-btn" data-page="1">1</button>
                    <button class="pagination-btn active" data-page="2">2</button>
                </div>
            </div>
        </div>
    </section>

    <!-- Acknowledgements -->
    <section class="acknowledgements">
        <div class="container">
            <h2 class="section-header">Acknowledgements</h2>
            <p>
                We thank our colleagues at the University of Washington and Carnegie Mellon University for helpful discussions throughout this project.
            </p>
        </div>
    </section>

    <!-- BibTeX -->
    <section class="bibtex">
        <div class="container">
            <h2 class="section-header">BibTeX</h2>
            <div class="bibtex-block">
@article{soraki2025objectforesight,
    title={ObjectForesight: Predicting 3D Object Trajectories from Human Videos},
    author={Soraki, Rustin and Bharadhwaj, Homanga and Farhadi, Ali and Mottaghi, Roozbeh},
    journal={arXiv preprint},
    year={2025},
    url={https://objectforesight.github.io}
}
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <p>Website template inspired by <a href="https://www.chenbao.tech/handsonvlm/">HandsOnVLM</a> and <a href="https://nerfies.github.io/">Nerfies</a>.</p>
    </footer>

    <script>
        // Dataset tab switching
        document.querySelectorAll('.demo-tab').forEach(tab => {
            tab.addEventListener('click', () => {
                const dataset = tab.dataset.dataset;

                // Update active tab
                document.querySelectorAll('.demo-tab').forEach(t => t.classList.remove('active'));
                tab.classList.add('active');

                // Hide all pages, show first page of selected dataset
                document.querySelectorAll('.demo-page').forEach(page => {
                    page.classList.remove('active');
                });
                const firstPage = document.querySelector(`.demo-page[data-dataset="${dataset}"][data-page="1"]`);
                if (firstPage) firstPage.classList.add('active');

                // Pause all videos
                document.querySelectorAll('.demo-page video').forEach(v => v.pause());
            });
        });

        // Pagination
        document.querySelectorAll('.pagination-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                const page = btn.dataset.page;
                const pagination = btn.closest('.pagination');
                const dataset = pagination.dataset.dataset;

                // Hide all pages of this dataset
                document.querySelectorAll(`.demo-page[data-dataset="${dataset}"]`).forEach(p => {
                    p.classList.remove('active');
                });

                // Show selected page
                const targetPage = document.querySelector(`.demo-page[data-dataset="${dataset}"][data-page="${page}"]`);
                if (targetPage) targetPage.classList.add('active');

                // Update pagination buttons on all pages of this dataset
                document.querySelectorAll(`.pagination[data-dataset="${dataset}"] .pagination-btn`).forEach(b => {
                    b.classList.toggle('active', b.dataset.page === page);
                });

                // Pause all videos
                document.querySelectorAll('.demo-page video').forEach(v => v.pause());
            });
        });
    </script>
</body>
</html>
